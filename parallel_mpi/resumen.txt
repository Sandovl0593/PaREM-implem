Por supuesto, Salvador. Aquí tienes una explicación técnica y profunda del contenido del artículo, incluyendo las métricas, las pragma utilizados, y cómo se realiza la comparación de los métodos paralelos frente a los secuenciales.

Resumen técnico profundo

El artículo presenta una implementación y análisis de métodos paralelos para ejecutar autómatas finitos deterministas (DFA), con enfoques enfocados en diferentes propiedades del autómata, especialmente la propiedad de k-localidad. La motivación principal es acelerar el proceso de reconocimiento de cadenas mediante el uso de hardware paralelo, incluyendo arquitecturas como clusters de estaciones de trabajo (MPI) y multiprocesadores compartidos (OpenMP, SMP, COW).

Se estudian principalmente:

- Métodos universales para DFA paralelos: que dividen la entrada en bloques (de acuerdo a block data decomposition) y run en paralelo, pero cuya complejidad depende en gran medida del número de estados ∣Q∣.

- Métodos específicos para autómatas k-local o synchronizing: que aprovechan la existencia de palabras de sincronización para sincronizar el estado inicial en cada procesador sin precomputar posibles estados iniciales, reduciendo la dependencia en ∣Q∣.

Métricas principales

1. Tiempo de ejecución (T):

- Se mide tanto para versiones secuenciales como paralelas.

- Especificamente, se presentan gráficas del tiempo en función del tamaño de la entrada y del número de procesadores.

1. Speedup (Sp​):
- Indicador de cómo el tiempo de ejecución paralelo mejorea respecto al secuencial:
Sp​=Tpar​Tsec​​

2. Dependencia del rendimiento respecto al ratio ∣P∣/∣Q∣:

- Evaluación clave para entender cuándo la paralelización es eficiente. La gráfica de la figura 5 en la página 11 muestra cómo varía el tiempo de ejecución en función de ∣P∣/∣Q∣.

1. Overheads de sincronización y comunicación:

- Se mencionan los costos de acceder a memoria compartida, sincronizar procesos, y realizar reducciones paralelas (binary reduction) en la comunicación entre procesadores.

PRAGMAS y tecnologías (paralelismo y sincronización)

- MPI:

- Se usa para el paralelismo en clústeres de trabajo (ejemplo en la sección 4.1, “Star”).

- La división de entrada y la comunicación entre nodos se gestionan mediante funciones MPI, con MPI_Send, MPI_Recv, barreras (MPI_Barrier) y reducciones, particularmente en el método de binary reduction descrito en la sección 3.3.

- Los bloque de entrada se distribuyen por bloques usando block data decomposition (figura 3 en pág. 8, “second method”).

- OpenMP:

- Se emplea para simulaciones en multiprocesadores compartidos (Altix), facilitando la utilización de hilos y memoria compartida.

- Se aprovecha para paralelizar las operaciones en tablas de transición y la ejecución del autómata en diferentes bloques.

- Overheads:

- Se mencionan los efectos del cache coherency, acceso a memoria compartida y la sobrecarga en la comunicación en arquitecturas SMP y COW (page 10, sección 4.1). En algunos casos, esto hace que la versión paralela sea más lenta que la secuencial, especialmente en las configuraciones con alta sobrecarga de comunicación y acceso a memoria compartida (figura 4).

Comparación de métodos

- Método general DFA (página 11, sección 5.2.1 y T7):

- La complejidad de ejecución en paralelo está dada por:
O(∣P∣∣Q∣n​+log∣P∣+∣Q∣log∣P∣)

- La primera parte, ∣P∣∣Q∣n​, corresponde a procesar la entrada dividida entre los procesadores y la computación de estados posibles.

- La parte log∣P∣ proviene del coste de sincronización y reducción en paralelo (coprocesamiento y combinaciones en árbol binario).

- La última parte, ∣Q∣log∣P∣, corresponde a la costos de reducción binaria de resultados parciales.

- Método usando k-locality y autómatas sincronizantes (T2, T4):

- Aprovecha la existencia de palabras sincronizantes que llevan al mismo estado final sin depender mucho del estado actual previo.

- Esto elimina la necesidad de precomputar posibles estados iniciales en cada procesador, y reduce la dependencia en ∣Q∣, lo que mejora la escalabilidad y rendimiento para autómatas con propiedades de sincronización.

- Experimentos de rendimiento:

- En una máquina Altix (figura 4 y 5), la versión paralela del DFA no siempre supera a la secuencial, principalmente por sobrecarga de comunicación y acceso a memoria.

- La eficiencia aumenta en ciertos rangos de ∣P∣/∣Q∣, pero se degrada cuando hay demasiados procesadores respecto a los estados del autómata.

Conclusión de los resultados

- La implementación demuestra que los métodos paralelos pueden ser útiles, especialmente cuando el autómata es k-local y permite sincronización sin precomputar todos los estados.

- La eficiencia dependerá en mucho del ratio ∣P∣/∣Q∣ y del hardware subyacente, con ventajas evidentes en arquitecturas con menor sobrecarga de memoria compartida y comunicación.

- La sobrecarga de sincronización y acceso a memoria puede eclipsar las mejoras del paralelismo, por lo que en la práctica, el método ideal requiere de propiedades especiales en los autómatas y de un ajuste cuidadoso del paralelismo.

¿Quieres que profundice en algún concepto específico, como la construcción de autómatas sincronizantes o la implementación de alguna pragma en particular?
El código secuencial en el documento comienza en la descripción general de la ejecución y en la implementación en la sección que corresponde a los métodos paralelos, específicamente en la subsección 5.2.1 ("Method of the parallel run of k-local DFA") y en la explicación de algoritmos como en la Tabla 5.1. Aunque no se proporciona un código explícito completo, el inicio de la paralelización se identifica en el método de división del texto de entrada entre procesadores, que sucede en la etapa de data decomposition, y en la sincronización mediante el uso de la sobreposición de los bloques con símbolos para asegurar el estado correcto al iniciar la ejecución en cada procesador.

Inicio de paralelización:
Empieza en la división del texto de entrada en bloques entre los procesadores, donde cada uno recibe la porción correspondiente y además extiende su bloque con los últimos k símbolos del bloque anterior para sincronizar el automáton correctamente. Esto se describe en la sección 5.2.1 y en la Figura 5.3.

Tipo de comunicación:
Se utiliza principalmente comunicación distribuida basada en punto a punto (point-to-point), ya que los procesadores intercambian los resultados parciales y las símbolos de frontera para sincronizarse y asegurar la coherencia del estado de automáton. La descripción del algoritmo indica que los procesos envían resultados parciales (por ejemplo, los estados finales alcanzados y los estados activos) al procesador siguiente, lo cual es una forma de comunicación punto a punto.

Asimismo, en la reducción de resultados, se menciona que se puede usar una reducción binaria (binary reduction), que es una comunicación colectiva allreduce. Esto se deduce del uso de algoritmos que combinan resultados en pasos logarítmicos (por ejemplo, la operación de reducción binaria que en la teoría de comunicaciones se implementa con MPI_Reduce o MPI_Allreduce).

Comunicación distribuida enfatizada:
Se hace énfasis en el uso de reducir los resultados, lo que implica mecanismos de comunicación como allreduce en MPI, que combina resultados de todos los procesos en uno o en todos los procesos (dependiendo de la función).

Granularidad:
La granularidad del algoritmo se puede inferir a partir del tamaño de los bloques de entrada que cada procesador recibe. La granularidad se relaciona con el tamaño de la tarea asignada a cada proceso y puede variar según el tamaño del bloque de entrada y la cantidad de símbolos k añadidos para sincronización. La concentración se da en el equilibrio entre el tamaño del bloque y el costo de comunicación y sincronización: si los bloques son demasiado pequeños, la comunicación se vuelve costosa; si son demasiado grandes, la eficiencia de paralelización puede disminuir.

Eficiencia:
Se define la eficiencia como la relación entre la velocidad de ejecución paralela y la velocidad secuencial, teniendo en cuenta la sobrecarga de comunicación y sincronización. En el análisis presentado en la página 11, se observa que la eficiencia disminuye a medida que el número de procesadores aumenta más allá de cierto punto, debido a conflictos en la comunicación y sobrecarga (bus y memoria compartida). Por lo tanto, la eficiencia depende también de la relación entre el número de procesadores |P| y el tamaño del automáton |Q|, y de la sobrecarga en las operaciones de reducción y comunicación.

Mejor pragma:
No se menciona explícitamente el uso de pragmas en el fragmento proporcionado, pero en el contexto de MPI y OpenMP se puede inferir que, para paralelizar las tareas a nivel de memoria compartida (OpenMP), sería recomendable usar pragmas como #pragma omp parallel para crear regiones paralelas y #pragma omp for para dividir los bucles.

Para comunicación distribuida en MPI, las funciones de reducción como MPI_Reduce, MPI_Allreduce y MPI_Bcast son las que se emplean. La elección del mecanismo (broadcast, reduce, scan) depende del paso específico del algoritmo: para sincronizar resultados parciales en una etapa de reducción, MPI_Reduce o MPI_Allreduce sería más adecuado.

Resumen:

- El proceso secuencial inicia en la lectura del input y en el cálculo para un automáton sin paralelismo explícito, pero el paralelismo se introduce en la División de bloques y en la sincronización de estados mediante intercambio de frontera (punto a punto).

- Se usa comunicación distribuida de tipo punto a punto y reacción de resultados usando reduction (posiblemente MPI_Allreduce).

- La granularidad depende del tamaño del bloque de entrada y del parámetro k para sincronización.

- La eficiencia se ve afectada por sobrecarga de comunicación y sincronización, disminuyendo cuando se aumenta la cantidad de procesadores más allá de un cierto equilibrio.

- En paralelización con MPI, las funciones MPI_Reduce y MPI_Allreduce son las más indicadas para simplificar la reducción de resultados.

¿Quieres que te resuma alguna parte en más detalle?
En el documento, la granularidad no se define explícitamente con los términos clásicos de "gruesa" o "fina", ni en esas categorías específicas. Sin embargo, se aborda indirectamente mediante la discusión sobre el tamaño de los bloques de entrada y el equilibrio de recursos entre cómputo y comunicación.

El análisis en el artículo, en particular en la sección 3.3.1 y en la discusión de la complejidad, indica que la elección del tamaño del bloque de entrada para cada procesador afecta la eficiencia y el rendimiento. Cuando los bloques son demasiado pequeños, los costos de comunicación, sincronización y el overhead de la gestión de tareas se vuelven predominantes. Cuando los bloques son demasiado grandes, se reduce el paralelismo efectivo. La recomendación de un "tamaño medio" es coherente con la conclusión general del artículo: que el tamaño de los bloques debe escogerse para equilibrar el tiempo de comunicación y el de cálculo.

Respecto a la fórmula que mencionas, Tcomputo​/Tcomunicacion​, en el paper no aparece expresamente como una relación formal o fórmula definida para granularidad. La discusión en el texto se enfoca en los términos de complejidad de tiempo de computo, como O(∣P∣∣Q∣n​), y en el tiempo de comunicación, como los costes asociados a operaciones de reducción o sincronización, como en las fórmulas de las Teoremas 3.3 y 3.4.

En conclusión, aunque el documento no presenta una fórmula directa exactamente en esa forma para definir la granularidad, sí aborda el concepto de equilibrio entre computación y comunicación a través de la selección del tamaño del bloque y la gestión del hardware y el algoritmo. La idea de que la granularidad ideal es aquella donde el tiempo de cómputo y comunicación usan recursos similares está alineada con la discusión en el paper, en particular en la sección de análisis y en las recomendaciones para balancear carga y comunicación.