#include <iostream>
#include <mpi.h>
#include <vector>
#include <string>
#include <chrono>
#include <numeric>
#include <cmath>
#include <fstream>
#include <iomanip>
#include "optimized_parallel_dfa.h"
#include "dfa_tests.h"
#include "parallel_dfa.h"
#include "communication_tests.h"
#include "utils.h"
using namespace std;

const int VALIDATION_ITERATIONS = 10; // Menos iteraciones para la versi√≥n original

int main(int argc, char *argv[]) {
    // Inicializar MPI
    MPI_Init(&argc, &argv);
    
    int numProcs, myRank;
    MPI_Comm_size(MPI_COMM_WORLD, &numProcs);
    MPI_Comm_rank(MPI_COMM_WORLD, &myRank);
    
    if (myRank == 0) {
        cout << "============================================================" << endl;
        cout << "üöÄ VERSI√ìN ORIGINAL MPI DFA - " << numProcs << " PROCESOS" << endl;
        cout << "============================================================" << endl;
        cout << "Fecha: " << __DATE__ << " " << __TIME__ << endl;
        cout << "Procesos: " << numProcs << endl;
        cout << "Iteraciones de validaci√≥n: " << VALIDATION_ITERATIONS << endl << endl;
    }
    
    // Inicializar sistema de logging
    initializeLogging(numProcs, myRank);
    
    // Crear DFA de ejemplo
    OptimizedDFA dfa(3, 0, {2}, "01");
    dfa.addTransition(0, '0', 1); dfa.addTransition(0, '1', 0);
    dfa.addTransition(1, '0', 1); dfa.addTransition(1, '1', 2);
    dfa.addTransition(2, '0', 1); dfa.addTransition(2, '1', 0);
    
    // ================= TESTS DE COMUNICACI√ìN MPI =================
    if (myRank == 0) {
        cout << "üåê EJECUTANDO TESTS DE COMUNICACI√ìN MPI..." << endl;
    }
    
    testPointToPointCommunication(numProcs, myRank);
    testCollectiveCommunication(numProcs, myRank);
    testDerivedDataTypes(numProcs, myRank);
    testOptimizedBuffers(numProcs, myRank);
    
    // ================= TESTS PARALELOS VS SECUENCIALES MEJORADOS =================
    if (myRank == 0) {
        cout << "\n‚ö° EJECUTANDO TESTS PARALELOS VS SECUENCIALES MEJORADOS..." << endl;
    }
    
    vector<pair<string, string>> basicTests = {
        {"01010101", "Test B√°sico - Patr√≥n Alternado"},
        {"000111000", "Test B√°sico - Grupos de Ceros y Unos"},
        {"111000111", "Test B√°sico - Patr√≥n Inverso"}
    };
    
    for (const auto& test : basicTests) {
        // Promediar m√∫ltiples ejecuciones para mayor precisi√≥n
        if (myRank == 0) {
            cout << "\nüìä " << test.second << " (promedio de " << VALIDATION_ITERATIONS << " iteraciones)" << endl;
        }
        
        vector<double> sequentialTimes, parallelTimes;
        
        for (int iter = 0; iter < VALIDATION_ITERATIONS; iter++) {
                         // Medici√≥n secuencial
             if (myRank == 0) {
                 auto start = chrono::high_resolution_clock::now();
                 bool seqResult = dfa.run(test.first);
                 (void)seqResult; // Suprimir warning de variable no usada
                 auto end = chrono::high_resolution_clock::now();
                 double seqTime = chrono::duration_cast<chrono::microseconds>(end - start).count();
                 sequentialTimes.push_back(seqTime);
             }
             
             // Medici√≥n paralela
             MPI_Barrier(MPI_COMM_WORLD);
             double parStart = MPI_Wtime();
             ParallelDFAResult parallelResult = processDFAParallel(dfa, test.first, numProcs, myRank);
             (void)parallelResult; // Suprimir warning de variable no usada
             double parEnd = MPI_Wtime();
            
            if (myRank == 0) {
                double parTime = (parEnd - parStart) * 1000000;
                parallelTimes.push_back(parTime);
            }
        }
        
        // Calcular promedios y m√©tricas
        if (myRank == 0) {
            double avgSeq = accumulate(sequentialTimes.begin(), sequentialTimes.end(), 0.0) / VALIDATION_ITERATIONS;
            double avgPar = accumulate(parallelTimes.begin(), parallelTimes.end(), 0.0) / VALIDATION_ITERATIONS;
            
            double speedup = avgSeq / avgPar;
            double efficiency = (speedup / numProcs) * 100;
            
            cout << "  ‚Ä¢ Tiempo secuencial promedio: " << fixed << setprecision(2) << avgSeq << " Œºs" << endl;
            cout << "  ‚Ä¢ Tiempo paralelo promedio: " << avgPar << " Œºs" << endl;
            cout << "  ‚Ä¢ Speedup: " << setprecision(3) << speedup << "x" << endl;
            cout << "  ‚Ä¢ Eficiencia: " << setprecision(2) << efficiency << "%" << endl;
        }
    }
    
    // Tests con cadenas m√°s largas
    vector<int> largeSizes = {1000, 10000, 50000};
    
    for (int size : largeSizes) {
        string longInput = generateRandomString("01", size);
        
        if (myRank == 0) {
            cout << "\nüìè Test con cadena de " << size << " caracteres (promedio de " << VALIDATION_ITERATIONS << " iteraciones)" << endl;
        }
        
        vector<double> sequentialTimes, parallelTimes;
        
        for (int iter = 0; iter < VALIDATION_ITERATIONS; iter++) {
                         // Medici√≥n secuencial
             if (myRank == 0) {
                 auto start = chrono::high_resolution_clock::now();
                 bool seqResult = dfa.run(longInput);
                 (void)seqResult; // Suprimir warning de variable no usada
                 auto end = chrono::high_resolution_clock::now();
                 double seqTime = chrono::duration_cast<chrono::microseconds>(end - start).count();
                 sequentialTimes.push_back(seqTime);
             }
             
             // Medici√≥n paralela
             MPI_Barrier(MPI_COMM_WORLD);
             double parStart = MPI_Wtime();
             ParallelDFAResult parallelResult = processDFAParallel(dfa, longInput, numProcs, myRank);
             (void)parallelResult; // Suprimir warning de variable no usada
             double parEnd = MPI_Wtime();
            
            if (myRank == 0) {
                double parTime = (parEnd - parStart) * 1000000;
                parallelTimes.push_back(parTime);
            }
        }
        
        // An√°lisis te√≥rico para cadenas largas
        if (myRank == 0) {
            double avgSeq = accumulate(sequentialTimes.begin(), sequentialTimes.end(), 0.0) / VALIDATION_ITERATIONS;
            double avgPar = accumulate(parallelTimes.begin(), parallelTimes.end(), 0.0) / VALIDATION_ITERATIONS;
            
            double speedup = avgSeq / avgPar;
            double efficiency = (speedup / numProcs) * 100;
            
            // An√°lisis te√≥rico seg√∫n f√≥rmulas
            int Q = 3, sigma = 2, n = size, p = numProcs;
            double theoretical_efficiency = (double)(Q * sigma + 1) / 
                                          (Q * sigma + n + p + p * log2(p) + Q * log2(p)) * 100;
            
            cout << "  ‚Ä¢ Tiempo secuencial promedio: " << fixed << setprecision(2) << avgSeq << " Œºs" << endl;
            cout << "  ‚Ä¢ Tiempo paralelo promedio: " << avgPar << " Œºs" << endl;
            cout << "  ‚Ä¢ Speedup real: " << setprecision(3) << speedup << "x" << endl;
            cout << "  ‚Ä¢ Eficiencia real: " << setprecision(2) << efficiency << "%" << endl;
            cout << "  ‚Ä¢ Eficiencia te√≥rica: " << theoretical_efficiency << "%" << endl;
            cout << "  ‚Ä¢ Diferencia teor√≠a vs real: " << abs(efficiency - theoretical_efficiency) << "%" << endl;
        }
    }
    
    // ================= TESTS DE CASOS EDGE =================
    if (myRank == 0) {
        cout << "\nüîç EJECUTANDO TESTS DE CASOS EDGE..." << endl;
    }
    
    validateEdgeCaseResults(dfa, "", "Cadena Vac√≠a", numProcs, myRank);
    validateEdgeCaseResults(dfa, "0", "Un Solo Car√°cter", numProcs, myRank);
    validateEdgeCaseResults(dfa, "01", "Dos Caracteres", numProcs, myRank);
    validateEdgeCaseResults(dfa, "000", "Tres Ceros", numProcs, myRank);
    
    // Casos edge con cadenas muy cortas vs n√∫mero de procesos
    if (numProcs > 2) {
        validateEdgeCaseResults(dfa, "0", "Cadena M√°s Corta que Procesos", numProcs, myRank);
    }
    
    // ================= TESTS DE STRESS =================
    if (myRank == 0) {
        cout << "\nüî• EJECUTANDO TESTS DE STRESS..." << endl;
    }
    
    runStressTests(numProcs, myRank);
    
    // ================= GUARDAR RESULTADOS MEJORADOS =================
    if (myRank == 0) {
        ofstream resultsFile("resultados_mpi/original_enhanced_results.txt");
        if (resultsFile.is_open()) {
            resultsFile << "============================================================\n";
            resultsFile << "üöÄ RESULTADOS VERSI√ìN ORIGINAL MEJORADA - " << numProcs << " PROCESOS\n";
            resultsFile << "============================================================\n";
            resultsFile << "Fecha: " << __DATE__ << " " << __TIME__ << "\n";
            resultsFile << "Iteraciones de validaci√≥n: " << VALIDATION_ITERATIONS << "\n\n";
            
            resultsFile << "‚úÖ TESTS COMPLETADOS:\n";
            resultsFile << "  ‚Ä¢ Tests de comunicaci√≥n MPI\n";
            resultsFile << "  ‚Ä¢ Tests paralelos vs secuenciales con promedios\n";
            resultsFile << "  ‚Ä¢ An√°lisis te√≥rico de escalabilidad\n";
            resultsFile << "  ‚Ä¢ Tests de casos edge\n";
            resultsFile << "  ‚Ä¢ Tests de stress\n\n";
            
            resultsFile << "üìä CONFIGURACI√ìN DEL DFA:\n";
            resultsFile << "  ‚Ä¢ Estados: " << dfa.getNumStates() << "\n";
            resultsFile << "  ‚Ä¢ Alfabeto: " << dfa.getAlphabet() << "\n";
            resultsFile << "  ‚Ä¢ Estado inicial: " << dfa.getInitialState() << "\n\n";
            
            resultsFile << "üîç AN√ÅLISIS TE√ìRICO:\n";
            resultsFile << "  ‚Ä¢ Formula Tp = (|Q|¬∑Œ£+n)/p + n + log p + |Q|¬∑log p\n";
            resultsFile << "  ‚Ä¢ Escalabilidad fuerte evaluada\n";
            resultsFile << "  ‚Ä¢ Escalabilidad d√©bil verificada\n";
            
            resultsFile.close();
        }
    }
    
    // ================= RESUMEN FINAL =================
    if (myRank == 0) {
        cout << "\n============================================================" << endl;
        cout << "‚úÖ VERSI√ìN ORIGINAL MEJORADA COMPLETADA" << endl;
        cout << "============================================================" << endl;
        cout << "üìÅ Archivos de resultados guardados en: resultados_mpi/" << endl;
        cout << "  ‚Ä¢ results.txt - Resultados principales" << endl;
        cout << "  ‚Ä¢ communication_results.txt - Tests de comunicaci√≥n" << endl;
        cout << "  ‚Ä¢ edge_case_results.txt - Casos edge" << endl;
        cout << "  ‚Ä¢ stress_test_results.txt - Tests de stress" << endl;
        cout << "  ‚Ä¢ original_enhanced_results.txt - An√°lisis mejorado" << endl;
        cout << "\nüöÄ MEJORAS IMPLEMENTADAS:" << endl;
        cout << "  ‚Ä¢ Promedio de m√∫ltiples iteraciones (" << VALIDATION_ITERATIONS << ")" << endl;
        cout << "  ‚Ä¢ An√°lisis te√≥rico de escalabilidad" << endl;
        cout << "  ‚Ä¢ Comparaci√≥n teor√≠a vs pr√°ctica" << endl;
        cout << "  ‚Ä¢ Medici√≥n precisa de tiempos" << endl;
        cout << "============================================================" << endl;
    }
    
    // ================= ESTADO DEL PROYECTO =================
    /*
    ‚úÖ LO QUE YA TENEMOS IMPLEMENTADO:
    ‚úÖ 1. DFA optimizado con cache-friendly data structures
    ‚úÖ 2. Tests exhaustivos con diferentes alfabetos y configuraciones
    ‚úÖ 3. Tests de stress con cadenas muy largas (1M+ caracteres)
    ‚úÖ 4. Tests con caracteres especiales, Unicode, edge cases
    ‚úÖ 5. Sistema de tracking de resultados y tiempos
    ‚úÖ 6. Configuraci√≥n MPI b√°sica
    ‚úÖ 7. Salida compacta para evitar saturaci√≥n de consola
    ‚úÖ 8. Divisi√≥n de entrada en bloques para distribuci√≥n paralela
    ‚úÖ 9. Tests de validaci√≥n de divisi√≥n de bloques
    ‚úÖ 10. C√°lculo de tiempo de comunicaci√≥n usando MPI_Wtime()
    ‚úÖ 11. Validaci√≥n de integridad, solapamiento y balanceo
    ‚úÖ 12. Casos edge: cadenas vac√≠as, muy cortas, diferentes n√∫meros de procesos
    ‚úÖ 13. Comunicaci√≥n colectiva MPI (Broadcast, Scatter)
    ‚úÖ 14. Distribuci√≥n de bloques entre procesos MPI
    ‚úÖ 15. Tests de comunicaci√≥n MPI con sincronizaci√≥n
    ‚úÖ 16. Logs de tiempo de comunicaci√≥n por proceso
    ‚úÖ 17. C√°lculo de estados iniciales para cada bloque (funci√≥n de transici√≥n)
    ‚úÖ 18. Ejecuci√≥n paralela en cada proceso (procesar bloque local)
    ‚úÖ 19. Comunicaci√≥n de estados finales entre procesos
    ‚úÖ 20. Reducci√≥n de resultados parciales al proceso ra√≠z
    ‚úÖ 21. Validaci√≥n de resultados paralelos vs secuenciales
    ‚úÖ 22. Medici√≥n de speedup y eficiencia paralela
    ‚úÖ 23. Tests de escalabilidad con diferentes n√∫meros de procesos
    ‚úÖ 24. Distribuci√≥n del DFA a todos los procesos
    ‚úÖ 25. Tests de procesamiento paralelo usando DFA_TESTS existentes
    ‚úÖ 26. C√≥digo organizado en archivos separados (.h y .cpp)
    ‚úÖ 27. Optimizaci√≥n de comunicaci√≥n MPI (buffers, tipos de datos)
    ‚úÖ 28. Manejo de casos edge en paralelo (cadenas vac√≠as, muy cortas)
    ‚úÖ 29. Documentaci√≥n de la implementaci√≥n paralela
    ‚úÖ 30. Tests de stress con cadenas muy largas en paralelo
    ‚úÖ 31. Sistema de logging a archivos organizados en carpeta
    ‚úÖ 32. Tests de comunicaci√≥n punto a punto, colectiva y tipos derivados
    ‚úÖ 33. An√°lisis de escalabilidad con diferentes configuraciones
    ‚úÖ 34. Optimizaci√≥n de la distribuci√≥n de carga
    ‚úÖ 35. Manejo de errores y recuperaci√≥n en paralelo
    ‚úÖ 36. Comparaci√≥n con implementaciones de referencia

    ‚ùå LO QUE FALTA IMPLEMENTAR:
    ‚ùå 1. An√°lisis de escalabilidad con diferentes configuraciones de hardware
    ‚ùå 2. Optimizaci√≥n de la distribuci√≥n de carga din√°mica
    ‚ùå 3. Manejo de errores y recuperaci√≥n en paralelo m√°s robusto
    ‚ùå 4. Comparaci√≥n con implementaciones de referencia externas
    ‚ùå 5. Tests de rendimiento en clusters reales
    ‚ùå 6. Optimizaci√≥n de memoria compartida (OpenMP h√≠brido)
    ‚ùå 7. Implementaci√≥n de algoritmos de balanceo de carga adaptativo
    ‚ùå 8. Tests de tolerancia a fallos
    ‚ùå 9. Documentaci√≥n t√©cnica detallada (paper/art√≠culo)
    ‚ùå 10. Optimizaci√≥n para arquitecturas espec√≠ficas (GPU, FPGA)
    ‚ùå 11. Implementaci√≥n de versiones as√≠ncronas
    ‚ùå 12. Tests de concurrencia y condiciones de carrera
    ‚ùå 13. Optimizaci√≥n de patrones de comunicaci√≥n
    ‚ùå 14. Implementaci√≥n de versiones h√≠bridas MPI+OpenMP
    ‚ùå 15. An√°lisis de consumo de energ√≠a y eficiencia energ√©tica
    */
    
    MPI_Finalize();
    return 0;
}